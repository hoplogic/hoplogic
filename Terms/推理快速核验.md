# 推理快速核验 (VerifyFast)

对大模型生成的推理输出进行快速三阶段审计（因子拆解 -> 事实核查 -> 逻辑检查），用确定性评分公式量化可信度，输出结构化报告。

## 核心思路

LLM 的输出幻觉分两类：**事实性幻觉**（说了假话）和**推理幻觉**（逻辑不通）。VerifyFast 不依赖 LLM 自评打分，而是：

1. 先用 LLM 做**错误检测**（Phase 1-3），输出结构化的错误列表
2. 再用**确定性公式**（Phase 4）从错误计数直接算分，杜绝 LLM 在打分环节注水

## 三阶段审计协议

```
context_input + model_output
        |
        v
  Phase 1: 因子拆解
  - 提取原子事实 (claims, 最多5条)
  - 提取推理链 (deductions)
        |
   ┌────┴────┐
   v         v
Phase 2    Phase 3
事实核查    逻辑检查
   |         |
   v         v
factual    logic
_errors    _errors
   └────┬────┘
        v
  Phase 4: 确定性评分
  credit_score = f(high_count, low_count)
```

### 错误分类三级标准

| 级别 | 含义 | 处置 |
|------|------|------|
| 矛盾 | 与 context_input 直接冲突（数字相反、方向颠倒） | 报告为错误 |
| 无支撑 | context_input 中无任何提及，不可合理推出 | 报告为错误 |
| 合理推断 | 可从 context_input 已有数据归纳概括得出 | **不算错误** |

### Severity 判定

| Severity | 事实类 | 逻辑类 |
|----------|--------|--------|
| **High** | 与 context_input 方向相反，或数字偏差>10%且方向相反，足以改变核心结论 | 因果方向完全颠倒、阿谀奉承导致事实方向性扭曲，足以改变核心结论 |
| **Low** | 数字偏差<=10%且方向一致、无支撑的定性评价、过度概括 | 过度概括、缺少中间步骤但结论方向正确、概念替换但不改变结论方向 |

### 确定性评分公式

```
High >= 3           -> 1 分（彻底不可信）
High 1-2            -> 2 分（基本不可信）
Low >= 2, High == 0 -> 3 分（边界，无法准确判断）
Low == 1, High == 0 -> 4 分（基本没问题）
无错误               -> 5 分（肯定没问题）
```

评分参数集中在 `strategy.yaml` 中管理，Hop.py 只做执行编排。

## 分区评估框架

### 为什么不用精确匹配衡量准确率

5 级评分看似精细，但 LLM 审计本身有不确定性。在实际应用中，**用户真正关心的不是 1 分和 2 分的区别，而是"这份报告到底能不能信"**。因此评估时将 5 级评分归并为三个区（Band）：

| 区 | 分值 | 语义 | 用户决策 |
|----|------|------|----------|
| **BAD** | 1-2 | 不可信，存在严重错误 | 拒绝/人工复核 |
| **MID** | 3 | 边界，无法确定 | 谨慎使用，需补充信息 |
| **GOOD** | 4-5 | 可信，无严重问题 | 可直接采信 |

### 三类偏差的严重程度

| 偏差类型 | 示例 | 严重度 | 说明 |
|----------|------|--------|------|
| **区内偏差** | 期望 1 实际 2，或期望 4 实际 5 | 可接受 | 用户决策不变 |
| **边界偏差** | 期望 3 实际 4，或期望 4 实际 3 | 可容忍 | MID 区本身就是灰区 |
| **跨区搞混** | 期望 1-2 实际 4-5，或反之 | **致命** | 用户决策完全颠倒 |

**跨区搞混（BAD<->GOOD Confusion）是唯一不可接受的错误**。把不可信的报告判为可信（漏报），或把可信的报告判为不可信（误报），都会直接导致错误决策。

### 分区混淆矩阵

评估时使用 3x3 混淆矩阵（行=期望区，列=实际区）：

```
              Actual
              BAD(1-2)  MID(3)  GOOD(4-5)
Expected
  BAD(1-2)      TP_bad    边界      跨区!
  MID(3)        边界      TP_mid    边界
  GOOD(4-5)     跨区!     边界      TP_good
```

核心指标：**跨区搞混率 = (BAD->GOOD + GOOD->BAD) / Total**

目标：**跨区搞混率 < 5%**

### 基线测试结果 (v3, kimi-full, n=20)

```
              BAD(1-2)  MID(3)  GOOD(4-5)
  BAD(1-2)       8        0        1       <- 1例跨区
  MID(3)         0        0        4       <- 全部边界偏差
  GOOD(4-5)      0        1        6       <- 1例边界偏差
```

| 指标 | 值 |
|------|-----|
| 分区准确率 | 14/20 = 70% |
| 跨区搞混 | 1/20 = 5% |
| 精确匹配 | 10/20 = 50% |
| +-1 匹配 | 18/20 = 90% |

跨区搞混的 1 例为 #13（major_factual）：期望 2 分，实际 4 分。LLM 将一个应判 High 的事实编造降级为 Low，导致评分从 BAD 区跳到 GOOD 区。

## LLM 配置与 Profile

### settings.yaml 结构

```yaml
defaults:
  max_tokens: 4000
  temperature: 0.1

llms:
  kimi-k2:
    base_url: "https://antchat.alipay.com/v1"
    model: "Kimi-K2-Instruct-0905"
    inference_engine: "aistudio-vllm"
  qwen3-235b:
    base_url: "https://antchat.alipay.com/v1"
    model: "Qwen3-235B-A22B"

profiles:
  kimi-full:          # 同模型执行+核验
    run: kimi-k2
    verify: kimi-k2
  cross-verify:       # 交叉核验：不同模型
    run: kimi-k2
    verify: qwen3-235b
```

### Profile 含义

每个 profile 定义一组 `run`（执行模型）+ `verify`（核验模型）的组合：

| Profile | run | verify | 特点 |
|---------|-----|--------|------|
| kimi-full | Kimi-K2 | Kimi-K2 | 速度快，同模型可能有一致性偏差 |
| cross-verify | Kimi-K2 | Qwen3-235B | 交叉核验降低同模型盲区 |

批量测试通过 `--profiles` 参数指定测试哪些 profile：

```bash
# 测试所有 profile
/batchhoptest VerifyFast verify_fast_20.jsonl

# 仅测试指定 profile
/batchhoptest VerifyFast verify_fast_20.jsonl --profiles kimi-full cross-verify
```

结果 JSONL 通过 `profile` 字段区分，分析时按 profile 分组对比。

### 策略配置与 Profile 联动（规划中）

`strategy.yaml` 支持 per-profile 策略覆盖：

```yaml
defaults:
  params:
    numeric_deviation_threshold: 0.10
    max_errors_per_phase: 3

profiles:
  kimi-full:
    params:
      numeric_deviation_threshold: 0.08   # 更严格的阈值
  cross-verify:
    params:
      max_errors_per_phase: 5             # 允许更多错误输出
```

这样不同 LLM 可以配套不同的审计策略参数，例如某些模型 severity 判定偏宽时收紧阈值。

## 测试数据规范

### 输入 JSONL 格式

```jsonl
{"id": 1, "tag": "clean", "difficulty": "easy", "description": "...", "context_input": "...", "model_output": "...", "expected_credit_score": 5}
```

### Tag 体系

| tag | 期望分 | 说明 | 期望区 |
|-----|--------|------|--------|
| clean | 5 | 完全忠实原文 | GOOD |
| minor_factual | 4 | 一个低严重度事实偏差 | GOOD |
| minor_logic | 4 | 一个低严重度逻辑问题 | GOOD |
| multi_minor | 3 | 多个低严重度错误叠加 | MID |
| major_factual | 2 | 一个高严重度事实错误 | BAD |
| major_logic | 2 | 一个高严重度逻辑错误 | BAD |
| severe_factual | 1 | 多个高严重度事实错误 | BAD |
| severe_logic | 1 | 多个高严重度逻辑错误 | BAD |
| severe_mixed | 1 | 事实+逻辑混合严重错误 | BAD |
| severe_fabrication | 1 | 大规模虚构 | BAD |
| sycophancy | 1 | 阿谀奉承导致方向性扭曲 | BAD |

### 分区覆盖要求

测试集应覆盖三个区且分布合理：

| 区 | 建议占比 | 说明 |
|----|----------|------|
| BAD (1-2) | 30-40% | 重点覆盖，跨区搞混的代价最高 |
| MID (3) | 15-25% | 边界case，检测灰区处理能力 |
| GOOD (4-5) | 35-50% | 基线覆盖，确保不误杀 |

## 已知问题与优化方向

### multi_minor 漏检

当前策略对多个低严重度错误叠加的场景检测偏弱（4 例全部偏高 1-2 分）。LLM 倾向于对单条错误给出"不算错误"的判定，导致多条小错误的累积效应被忽略。

可能的优化方向：
- 在 strategy.yaml 中增加 multi-error 敏感度提示
- 降低 exemption 的适用范围

### severity 降级

#13 案例中，LLM 将一个应判 High 的事实编造（985 录取率）判为 Low。这是跨区搞混的直接原因。

可能的优化方向：
- 在 factual severity 规则中增加"无中生有具体数字"必须判 High 的硬约束
- 考虑引入 reverse_verify 对 severity 判定做二次校验

### 跨 profile 对比

启用 cross-verify profile（不同模型交叉核验）后，观察跨区搞混率是否下降。不同模型的 severity 判定偏差可能互相补偿。
