# HOP vs Burr: Two Design Paths for LLM Application Frameworks

## Introduction

[Apache Burr](https://burr.apache.org/) is a general-purpose state machine application framework for building decision-making applications (agents, dialogue systems, simulations, etc.). HOP (High-Order Program) is a trustworthy agent programming paradigm for large models - controllable, reliable, and continuously evolving. It constrains LLM intelligence through structured program skeletons, eliminates hallucinations through verification loops, and achieves continuous evolution through progressive solidification. Both target LLM application development, but fundamentally answer different questions:

- **Burr**: Where is the application's state? What to do next?
- **HOP**: Is the LLM output correct? How to ensure it's correct?

This article systematically compares the two to explain HOP's core design philosophy.

---

## 1. HOP's Three Core Design Principles

### 1.1 Python is the Best Control Flow

HOP's first design choice is **not introducing control flow abstractions**.

Burr requires developers to model business logic as explicit state machines - defining Action nodes, declaring Transition conditions, configuring Guard expressions. The framework takes over control flow scheduling:

```python
# Burr: Framework-driven control flow
builder = ApplicationBuilder()
builder.with_actions(
    extract=extract_action,
    judge=judge_action,
    supplement=supplement_action,
)
builder.with_transitions(
    ("extract", "judge", default),
    ("judge", "supplement", when(status="UNCERTAIN")),
    ("judge", "done", when(status="OK")),
    ("supplement", "judge", default),
)
app = builder.build()
app.run(...)
```

HOP believes Python's native `if`/`for`/`while`/`await`/`try` are the best control flow. All business logic is expressed in standard Python, with the framework only handling LLM call reliability:

```python
# HOP: Python is the control flow
async with hop_proc.session() as s:
    status, facts = await s.hop_get(task="extract facts", context=doc)
    if status != HopStatus.OK:
        return {"error": "extraction failed"}

    for fact in facts:
        status, verdict = await s.hop_judge(task="verify", context=fact)
        if status == HopStatus.UNCERTAIN:
            status, verdict = await s.hop_get(task="supplement verification", context=fact)
        results.append({"fact": fact, "valid": verdict})
```

This isn't "simplification" - it's a deliberate architectural decision for three reasons:

**Expressiveness**. Any `if/else` nesting, `break`/`continue` in loops, exception handling, early returns are native Python syntax, no need to adapt to framework transition condition DSL. State machine DSL expressiveness is always a subset of Python.

**Debuggability**. Breakpoints go directly on business logic, call stack shows execution flow. No separation between "graph definition" and "actual execution" - code is flow, flow is code.

**LLM Generatability**. HOP programs (Hop.py) are automatically generated by large models through `/spec2code`. Python has the highest representation in LLM training data, generating and modifying Python control flow is LLM's strongest capability. If control flow uses framework-specific DSL, the model needs additional learning of API syntax, understanding graph topology to runtime behavior mapping, significantly reducing generation quality.

### 1.2 Framework Handles Reliability, Not Business Logic

HOP's second design choice is **focusing framework capabilities on LLM reliability**.

As a general state machine framework, Burr provides orchestration capabilities: state transitions, persistence, visualization, hooks. But it doesn't care about LLM output correctness - that's entirely the user's Action responsibility. To implement verification in Burr, each Action needs repetitive code:

```python
# Verification implementation in Burr: each Action must write it
@action(reads=["context"], writes=["result"])
def extract_with_verify(state):
    for attempt in range(3):
        result = call_llm(state["context"])
        # Manual verification logic
        verify_result = call_another_llm(f"verify: {result}")
        parsed = json.loads(verify_result)
        if parsed["status"] == "OK":
            return state.update(result=result)
        # Manual retry feedback
        feedback = parsed["reason"]
    return state.update(result=None, error="all retries failed")
```

HOP internalizes the verification loop as framework capability. One `hop_get` call automatically completes:

1. Prompt construction (with security token filtering)
2. LLM execution (with transport layer retry + exponential backoff)
3. Structured output parsing
4. Verification (reverse verification / forward cross-verification / tool verification / scenario verification)
5. On verification failure, inject reason as feedback into conversation history, trigger LLM retry
6. 4 semantic status returns (OK / FAIL / UNCERTAIN / LACK_OF_INFO)
7. Full chain statistics recording (success_rate, retry_count, execution_time)

User code only needs to care about "what to do", not "how to ensure it's correct":

```python
# HOP: One line call = execution + parsing + verification + retry + statistics
status, result = await s.hop_get(task="extract facts", context=doc)
```

### 1.3 HOP Programs are for Large Model Execution

HOP's full name is High-Order Program - high-order program. It's not a regular script, but an engineering asset that combines precise program logic, domain knowledge definitions, and multi-layer verification mechanisms. HOP's positioning is **the machine-executable version of SOP (Standard Operating Procedure) in the large model era**.

This means HOP program lifecycle is:

```
Domain expert writes SOP → AI converts to HopSpec → AI generates pseudocode → AI generates Hop.py → Engine executes
```

Throughout this pipeline, control flow is generated by AI, executed by engine, iteratively modified by AI. Python as control flow language serves three roles simultaneously:

- **Domain experts** can review and understand processes through pseudocode
- **AI programming assistants** can generate and modify code with high quality
- **Python runtime** executes directly without additional graph compilation or interpretation layers

Burr's state machine model introduces friction in this chain - SOP to state graph mapping isn't intuitive, AI generating graph definitions has lower reliability than generating Python functions, domain experts have harder time reviewing declarative transition conditions.

---

## 2. System Comparison

### 2.1 Design Philosophy

| | HOP | Burr |
|--|-----|------|
| Core Problem | LLM output reliability | Application state orchestration |
| Control Flow | Native Python | Framework declarative state machine |
| Framework Responsibility | Verification loop + reliability statistics | State transition + persistence + visualization |
| Target Users | LLM task developers (including AI code generation) | General application developers |

### 2.2 Core Capabilities

| Capability | HOP | Burr |
|------|-----|------|
| LLM Verification Loop | Built-in 5 types of verifiers | None |
| Verification-driven Retry (with feedback) | Built-in | None |
| Semantic Status (OK/FAIL/UNCERTAIN/LACK_OF_INFO) | Built-in | None |
| Cross-operator conversation history (checkpoint+finally folding) | Built-in | None (user-managed) |
| Reliability statistics (success_rate, retry, timing) | Built-in | None |
| Pause/resume at any node | Not supported | Supported |
| Visual tracing UI | None | Built-in Web UI |
| Non-LLM scenarios | Not applicable | Applicable |

### 2.3 Programming Model

| | HOP | Burr |
|--|-----|------|
| Code Style | Imperative `await s.hop_get(...)` | Declarative Action + Transition |
| One operator call includes | Execution+parsing+verification+retry+statistics | User-defined (framework only schedules) |
| Modifying flow | Change one line of Python | Change Action + Transition + Guard |
| LLM generation friendliness | High (standard Python) | Low (need to learn framework API) |
| IDE support | Complete (jump, type checking, breakpoints) | Graph definition separate from runtime |

### 2.4 Architecture Layers

**HOP Three-layer Architecture**:

```
User code (Hop.py)          ← Python control flow, AI generatable
    |
HopSession                 ← Session boundary: conversation history, state, statistics, persistence
    |
HopProc                    ← Operator: execution + verification + retry loop (stateless, shareable)
    |
LLM                        ← Transport layer: connection reuse, engine adaptation, structured output
```

**Burr Architecture**:

```
User code                    ← Build Application
    |
Application                ← State machine runtime: scheduling, persistence, hooks
    |
Action + Transition        ← Node + transition conditions (user defines all logic)
    |
State                      ← Immutable state container
```

Key difference: HOP's middle layer (HopProc) has built-in verification loop semantics, user calling operator gets reliability guarantee. Burr's middle layer (Application) only does scheduling, reliability is entirely Action's responsibility.

### 2.5 Concurrency Model

| | HOP | Burr |
|--|-----|------|
| Recommended Approach | Single-thread asyncio + gather multiple sessions | Single app single-thread; multiple apps multi-thread |
| Parallel Mode | Multiple sessions share HopProc | MapStates/MapActions (map-reduce) |
| Thread Safety | Lock-protected shared resources (GLOBAL_STATS, StateStore) | Immutable State naturally safe |

### 2.6 Exception Handling

| | HOP | Burr |
|--|-----|------|
| Error Model | Pure return value `(HopStatus, result)`, never throws exceptions to user | Python standard exception propagation |
| Transport/Verification Failure Distinction | `error_type` field distinguishes | No built-in distinction |
| Failure Isolation | Persistence/statistics failures silently swallowed, don't affect business | Depends on hook implementation |

### 2.7 Persistence and Observability

| | HOP | Burr |
|--|-----|------|
| Persistence Model | StateStore Protocol + JSONL append write | BaseStatePersister (PostgreSQL/Redis/SQLite etc.) |
| Recovery Granularity | Operator-level step recording | State machine node-level snapshots (can rollback to any historical node) |
| Monitoring | ExecutionStats API + JSONL | Built-in Web UI + OpenTelemetry |
| State Mutability | Mutable dataclass | Immutable (functional updates) |

Burr is more mature in persistence and observability. Immutable State enables snapshots and rollbacks at any point in time, built-in UI works out of the box. HOP's persistence is lighter, targeting batch processing scenarios.

---

## 3. Applicable Scenarios

| Scenario | Recommendation | Reason |
|------|------|------|
| High-reliability LLM tasks in professional domains (finance/medical/security) | HOP | Built-in verification loop, framework-level reliability guarantee |
| Machine-executable version of SOP (AI generation + iteration) | HOP | Python control flow most friendly to AI generation |
| Batch LLM processing + reliability statistics | HOP | auto_record_status + ExecutionStats work out of the box |
| Long-lifecycle interactive agents (pause/resume/rollback) | Burr | Immutable state + arbitrary node snapshot recovery |
| Need visual process audit/compliance | Burr | Built-in Web UI + state machine naturally visual |
| Non-LLM decision-making applications | Burr | HOP deeply bound to LLM |
| Need integration with LangChain/LlamaIndex | Burr | Framework-agnostic design |

---

## 4. Summary

HOP and Burr represent two different LLM application framework design paths:

**Burr Path**: General orchestration. Uses declarative state machines to manage application control flow and state, providing persistence, visualization, hooks and other infrastructure. LLM is just an implementation detail inside Actions, the framework makes no guarantees about LLM output correctness.

**HOP Path**: Trustworthy execution. Leaves control flow to Python (and AI that generates Python), focuses framework efforts on LLM-specific problems - verification loops, retry feedback, reliability statistics.

The two are not mutually exclusive. In theory, you could call HOP operators inside Burr Actions to get dual capabilities of state machine orchestration + verification reliability. But for HOP's target scenarios - batch processing, real-time analysis, SOP automation - Python control flow is sufficient, state machine layer doesn't add necessary capabilities.